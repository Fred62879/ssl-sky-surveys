{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from utils.load_trained_model import load_model_from_checkpoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading checkpoint checkpoints/pretrained_paper_model.pth.tar\n",
      "INFO:root:Printing a pretrained model without FC layers\n",
      "INFO:root:Chekpoint loaded. Checkpoint epoch 101\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'checkpoints/pretrained_paper_model.pth.tar'\n",
    "model = load_model_from_checkpoint(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File('datasets/sdss_w_specz_valid.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys: <KeysViewHDF5 ['ObjID', 'dec', 'e_bv', 'images', 'ra', 'specObjID', 'specz_redshift', 'specz_redshift_err']>\n",
      "Images shape: (102993, 5, 107, 107)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data keys:\", data.keys())\n",
    "print(\"Images shape:\", data['images'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain representations for the first 16 imges in the validation dataset.\n",
    "Note that we train the model on 64x64 images while in this example (for illustration purposes) we apply the model on the 107x107 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    reps = model(torch.tensor(data['images'][0:16]).to(torch.cuda.current_device())).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2048)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.7.1-gpu",
   "language": "python",
   "name": "pytorch-1.7.1-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
